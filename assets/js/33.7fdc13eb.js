(window.webpackJsonp=window.webpackJsonp||[]).push([[33],{391:function(t,a,s){"use strict";s.r(a);var h=s(42),e=Object(h.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h3",{attrs:{id:"hash-字典"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#hash-字典"}},[t._v("#")]),t._v(" hash（字典）")]),t._v(" "),s("p",[t._v("dict （字典），类似于Java 中的 map，k-v 型的数据结构，每个键都是唯一的。底层分别采用了 ziplist 和 hashtable 作为底层数据结构。当元素过少时，使用 ziplist，否则使用 hashtable")]),t._v(" "),s("h5",{attrs:{id:"_3-1-ziplist-在-hash-对象"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-ziplist-在-hash-对象"}},[t._v("#")]),t._v(" 3.1 ziplist 在 hash 对象")]),t._v(" "),s("p",[t._v("每当有 hash 对象需要放入键值对时，就会依次先插入到 ziplist 的尾部，首先插入 key 值，然后插入 value 值，使得 key 和 value 紧挨着，示例如下：")]),t._v(" "),s("div",{staticClass:"language-shell extra-class"},[s("pre",{pre:!0,attrs:{class:"language-shell"}},[s("code",[t._v("Tian:"),s("span",{pre:!0,attrs:{class:"token operator"}},[s("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[t._v("0")]),t._v(">")]),t._v("hmset person name andy age "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("18")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"OK"')]),t._v("\nTian:"),s("span",{pre:!0,attrs:{class:"token operator"}},[s("span",{pre:!0,attrs:{class:"token file-descriptor important"}},[t._v("0")]),t._v(">")]),t._v("object encoding person\n"),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ziplist"')]),t._v("\n")])])]),s("p",[s("img",{attrs:{src:"https://gitee.com/rule-liu/pic/raw/master/img/20211023185949.png",alt:""}})]),t._v(" "),s("blockquote",[s("p",[t._v("hash 对象的编码可以是 ziplist 或者 hashtable\n当 hash 对象可以同时满足一下条件，则为 ziplist 编码：")]),t._v(" "),s("ol",[s("li",[t._v("哈希对象保存的所有键值对的键和值的字符串长度都小于64字节")]),t._v(" "),s("li",[t._v("哈希对象保存的键值对数量小于 512个；\n不满足上述情况，否则使用 hashtable 编码。")])])]),t._v(" "),s("h5",{attrs:{id:"_3-2-hashtable-哈希表-结构"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-hashtable-哈希表-结构"}},[t._v("#")]),t._v(" 3.2 hashtable (哈希表)结构")]),t._v(" "),s("div",{staticClass:"language-c++ extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("// 节点结构\nstruct dictEntry {\n    void* key;\n    void* val;\n    // 链接下一个entry，解决 hash键 冲突\n    dictEntry* next;\n}\n\n// 字典结构\ntypedef struct dictht {\n    // 哈希表数组\n    dictEntry ** table;\n    // 哈希表大小\n    unsigned long size;\n    // 哈希表大小掩码，用于计算索引值，总是等于 size - 1\n    unsigned long sizemask;\n    // 该哈希表已有节点的数量\n    unsigned long used;\n} dictht;\n\n")])])]),s("p",[s("img",{attrs:{src:"https://gitee.com/rule-liu/pic/raw/master/img/20211021224239.png",alt:""}})]),t._v(" "),s("h5",{attrs:{id:"_3-3-字典数据结构"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-字典数据结构"}},[t._v("#")]),t._v(" 3.3 字典数据结构")]),t._v(" "),s("p",[t._v("结构中包含了两个 hashtable，通常情况下只有一个 ht[0] 是有值的，但是在 dict 扩容缩容的时，需要分配新的 hashtable 进行“渐进式搬迁”，rehashidx 记录 rehash 的进度，待搬迁结束后，旧的 ht[0] 被删除，新的 ht[1] 成为被使用的。一直反复交替，有点类似与垃圾回收的 survivor 区。")]),t._v(" "),s("div",{staticClass:"language-c++ extra-class"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("typedef struct dict {\n    // 类型特定的函数\n    dictType *type;\n    // 私有数据\n    void *privdata;\n    // 内部有两个 dictht 结构，也就是上面的 hash 表\n    dictht ht[2];\n    // rehash 索引，当 rehash 不再进行时，值为 -1\n    long rehashidx; \n    // 如果 > 0， rehash 暂停\n    int16_t pauserehash; \n}\n")])])]),s("p",[s("img",{attrs:{src:"https://gitee.com/rule-liu/pic/raw/master/img/20211021230632.png",alt:""}})]),t._v(" "),s("h5",{attrs:{id:"_3-4-hash-冲突"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-hash-冲突"}},[t._v("#")]),t._v(" 3.4 hash 冲突")]),t._v(" "),s("p",[t._v("redis 采用的链地址法来解决键冲突，每个 hash 节点都有个 next 指针，多个 hash 节点可以用链表组成一个单向的链表，类似于 Java 1.7 中的 hashmap 解决 hash 冲突的方法。冲突的节点，放在整个链表的头节点，复杂度O（1），如果用尾插法，那么就需要遍历到尾节点，然后进行指向。")]),t._v(" "),s("h5",{attrs:{id:"_3-5-扩容条件"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-扩容条件"}},[t._v("#")]),t._v(" 3.5 扩容条件")]),t._v(" "),s("ol",[s("li",[t._v("当 redis 没有执行 BGSAVE 或者 BGREWRITEAOF 命令，并且哈希表的负载因子大于等于 1，扩容的新数组是旧数组的2倍。")]),t._v(" "),s("li",[t._v("当 redis 正在做 BGSAVE 或者 BGREWRITEAOF 命令，为了减少内存页的过多分离，redis 不进行扩容, 除非哈希表的负载因子大于等于 5进行强制扩容。")])]),t._v(" "),s("blockquote",[s("p",[t._v("负载因子 = hash 表已保存节点数量 / hash 表大小 = ht[0].used / ht[0].size")])]),t._v(" "),s("h5",{attrs:{id:"_3-6-缩容条件"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-缩容条件"}},[t._v("#")]),t._v(" 3.6 缩容条件")]),t._v(" "),s("p",[t._v("当负载因子小于 0.1时，会进行缩容，也就是元素的个数低于数组长度的 10%。")]),t._v(" "),s("h5",{attrs:{id:"_3-7-rehash"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-7-rehash"}},[t._v("#")]),t._v(" 3.7 rehash")]),t._v(" "),s("p",[t._v("当 ht[0]为正在使用的 hash 表，")]),t._v(" "),s("ul",[s("li",[t._v("执行扩容时，ht[1]的空间大小为第一个 >= ht[0].used"),s("em",[t._v("2 的 2^n。例如 ht[0].used=5，5")]),t._v("2=10<=2^4(16)，所以 ht[1]的空间大小为 16。")]),t._v(" "),s("li",[t._v("执行缩容时，ht[1]的空间大小为第一个 >= ht[0].used 的 2^n。例如 ht[0].used=5，2^2(4)<=5<=2^3(8)，所以 ht[1]的空间大小为 8。")])]),t._v(" "),s("p",[t._v("最后释放 ht[0], 使其变为逻辑意义上的 ht[1]。")]),t._v(" "),s("h5",{attrs:{id:"_3-8-rehash-渐进式过程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-8-rehash-渐进式过程"}},[t._v("#")]),t._v(" 3.8 rehash 渐进式过程")]),t._v(" "),s("ol",[s("li",[t._v("为ht [ 1]分配空间，让字典同时持有ht[0]和ht[1]两个哈希表。")]),t._v(" "),s("li",[t._v("在字典中维持一个索引计数器变量rehashidx，并将它的值设置为0，表示rehash工作正式开始。")]),t._v(" "),s("li",[t._v("在rehash进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会顺带将ht[0]哈希表在rehashidx索引上的所有键值对rehash到ht[1]，当rehash工作完成之后，程序将rehashidx属性的值增一。")]),t._v(" "),s("li",[t._v("随着字典操作的不断执行，最终在某个时间点上，ht[0]的所有键值对都会被rehash 至ht [1]，这时程序将rehashidx属性的值设为-1，表示 rehash操作已完成。\n渐进式rehash的好处在于它采取分而治之的方式，将rehash键值对所需的计算工作均摊到对字典的每个添加、删除、查找和更新操作上，从而避免了集中式rehash而带来的庞大计算量。")]),t._v(" "),s("li",[t._v("补充: 当 rehash 过程中，字典进行查找操作时，首先会从ht[0]中查询，如果没有则会从ht[1]中，而新添加的键则会直接保存到ht[1]中。")])]),t._v(" "),s("blockquote",[s("p",[t._v("在每一次的 hset / hdel 指令会进行搬迁，如果没有指令触发，则会在定时任务中对字典进行主动的搬迁。")])])])}),[],!1,null,null,null);a.default=e.exports}}]);